# Content Review and Improvement Process

## Purpose

This process systematically reviews and improves written content through three assessment levels: document-level (content as unified system), section-level (section-by-section coherence), and line-level (line-by-line precision). Use this when asked to review, critique, or improve existing written material.

**Critical Requirements:**
- Load `AGENTS.md` into context before starting review (enforcement mechanisms, prohibited elements, required structure)
- Load `voice.md` into context before starting review (voice patterns, examples, verification)
- Apply AGENTS.md and voice.md standards throughout all assessment levels

## General Review Principle

- Present summary of findings first
- Work through observations one-by-one (never dump all at once)
- Get user input on each before proceeding
- Re-assess every 5 changes

## When to Use This Process

Use this process when the user asks you to:
- Review existing content
- Critique written material
- Improve a draft
- Assess quality of content
- Find issues or problems in writing
- Provide feedback on material

Do NOT automatically initiate this process after completing the six-step writing system. Only use this when explicitly requested.

# PART I: INITIAL ASSESSMENT

## Clarify Review Scope

Ask the user what aspects to focus on:

"What should this content review focus on?
- **1. Full Content Review** (recommended): Quality, coherence, AGENTS.md + voice.md standards alignment, transformation potential
- **2. Content Quality Only**: Writing quality and coherence without AGENTS.md/voice.md assessment
- **3. Standards Alignment Only**: AGENTS.md (enforcement mechanisms, structure) + voice.md (voice/tone principles) without content assessment
- **Other**: Describe your specific focus"

After user confirms focus, proceed to **Salvageability Assessment**.

## Salvageability Assessment

Before detailed assessment, evaluate whether the material can be improved incrementally or requires complete rewrite:

**Salvageable (proceed with incremental review):**
- Core structure serves the content adequately
- Issues are localized and can be fixed without cascading changes
- Fundamental concepts are sound but need refinement
- Voice/style issues are correctable through targeted edits

**Requires Rewrite:**
- Fundamental structural problems that can't be fixed incrementally
- Conceptual framework is incoherent or contradictory at its foundation
- Multiple competing approaches creating irreconcilable tension
- Attempting incremental fixes would create escalating problems
- Material would be clearer if rebuilt systematically from concepts

If material requires rewrite, recommend:
"This material needs systematic rewriting rather than incremental improvement. The [specific issues] create foundational problems that would escalate through iteration. I recommend mapping core concepts into new material written systematically using the six-step writing process. Would you like me to proceed with that approach instead?"

If salvageable, proceed to **Document-Level Content Assessment**.

# PART II: CONTENT ASSESSMENT LEVELS

## Document-Level Content Assessment

**Purpose**: Assess the document as a unified body of work, not just collection of sections. This catches synthesis opportunities, conceptual clustering, and structural patterns that section-by-section review misses.

This is the MOST CRITICAL assessment for longer documents (>1000 words). It determines whether content is greater than the sum of its parts.

**Synthesis Assessment:**
- Where is the same concept explored multiple times from different angles?
- Could multiple treatments synthesize into one stronger statement?
- Are there redundant explanations that should unify?
- Which concepts appear scattered that should consolidate?
- If sections are reordered, will technical terms still be grounded before use?
- Where might synthesis or restructuring move terms before their definitions?

**Thematic Unity:**
- Does this tell a story or list concepts?
- What's the narrative arc (if any)?
- Do sections create causal momentum (A enables B enables C) or just stack up (also A, also B, also C)?
- Is there rising tension and resolution, or flat sequencing?

**Conceptual Architecture:**
- What patterns emerge when you map all concepts together?
- Should scattered concepts cluster to reveal deeper structure?
- What's the dependency chain between concepts?
- What conceptual relationships are implied but not made explicit in structure?

**Cognitive Layering (for progressive content):**

For content where understanding builds across sections:
- What mental model does each section construct?
- Does each section's mental model require only what came before?
- Where might readers skip ahead and get confused from missing prerequisites?
- Where are cognitive bridges needed at transitions?

Skip for modular, reference, or single-section content.

**Depth vs Breadth:**
- Is this trying to cover too much without sufficient depth?
- Which concepts deserve more depth vs which are complete?
- What could be cut entirely to strengthen what remains?
- Where does breadth dilute impact?

**Transformation Potential:**
- Does this transform understanding or just inform?
- After reading, what changes for the reader?
- What makes this worth the time investment?
- Is this teaching a skill/framework or just describing concepts?

**Completeness-Conciseness Balance:**

Always assess both dimensions - these are peer criteria, not alternatives. Use these criteria both to assess the document (identifying what needs work) and to filter your recommendations (ensuring proposed changes strengthen the work).

**Conciseness Assessment:**
- What can be cut without loss of substance?
- Where does redundancy dilute rather than reinforce?
- Which sentences can consolidate (3→1)?
- What scaffolding exists to explain vs. deliver value?
- Are there sections that introduce/explain other sections rather than delivering value themselves?

**Completeness Assessment:**
- What's distinctly missing that should exist?
- Where do concepts need grounding through examples?
- What logical gaps would confuse readers?
- Where does brevity sacrifice necessary clarity?
- What's implied but not made explicit that needs to be?

For each observation, assess BOTH: Would addition strengthen this, or would cutting/consolidating strengthen it more?

**Assessment Process:** Map the entire conceptual architecture of the document. As you identify synthesis opportunities, clustering possibilities, depth/breadth trade-offs, and scaffolding sections, track them in a working list (mental notes, scratch pad, or internal tracking). Once the full document-level assessment is complete, prioritize all tracked observations by potential impact on document coherence and power, then proceed to **Section-Level Content Assessment**.

**Prioritization criteria for document-level content observations:**
1. Major synthesis opportunities (multiple treatments → one powerful statement)
2. Structural scaffolding that should be cut
3. Conceptual clustering that would reveal deeper patterns
4. Depth vs breadth trade-offs that affect document impact
5. Thematic coherence improvements

## Section-Level Content Assessment

**Structural Coherence:**
- Does the overall argument flow logically from beginning to end?
- Are sections properly sequenced or does the order create confusion?
- Does the structure serve the content or fight against it?

**Completeness:**
- What's distinctly missing that should exist based on the existing content?
- What novel ideas are implied but not developed?
- What gaps exist in the conceptual framework?

**Internal Consistency:**
- Do sections contradict each other in substance (not just style)?
- Are key terms or concepts used inconsistently?
- Do examples contradict the principles they're meant to illustrate?

**Conceptual Integrity:**
- Is the core thesis clear and maintained throughout?
- Do all sections support the main argument or do some wander?
- Are there competing frameworks that create conceptual tension?

**Assessment Process:** Review the entire document against all criteria above. As you identify issues, track them in a working list (mental notes, scratch pad, or internal tracking). Once the full section-level assessment is complete, compile and prioritize all tracked observations by urgency and impact, then proceed to **Recommendation Quality Filter**.

**Prioritization criteria:**
1. Foundational issues that affect multiple other areas
2. Internal contradictions
3. Structural problems
4. Missing concepts or gaps in framework
5. Clarity and refinement issues

# PART III: PRESENTING AND IMPLEMENTING CHANGES

## Recommendation Quality Filter

**Critical Safeguard**: Before presenting any observation to the user, pass it through this filter to ensure recommendations strengthen rather than dilute content.

**For every potential recommendation, test BOTH dimensions:**

**Conciseness Test (always apply):**
- If recommending addition: Could synthesis/consolidation solve this instead?
- If recommending addition: Does this add signal or noise?
- If recommending addition: Is this making something explicit that's better left implicit?
- If recommending addition: Would this become scaffolding or throat-clearing?
- If recommending removal: Does this cut substance or just scaffolding?
- If recommending removal: Where does redundancy dilute rather than reinforce?

**Completeness Test (always apply):**
- If recommending removal: Does brevity sacrifice necessary clarity?
- If recommending removal: Are we cutting a gap-filler that should stay?
- If recommending addition: Does this fill a genuine gap or create redundancy?
- If recommending addition: Would document be weaker without this addition, or just different?

A recommendation passes if the answer clearly strengthens the work; if uncertain, don't present it.

**Synthesis Over Addition:**
When you identify a gap or weakness, first check: Can existing content synthesize to fill this gap? Only recommend addition if synthesis isn't possible.

**Both Tests Must Pass:**
A recommendation must strengthen BOTH conciseness AND completeness, or improve one without harming the other. If a change improves conciseness but creates confusion (completeness suffers), don't recommend it. If it fills a gap but creates redundancy (conciseness suffers), find a better solution.

**The Ultimate Filter:**
Before presenting a recommendation, ask: "Am I suggesting this because it would strengthen the work, or because it's something I noticed could be different?"

Only present recommendations that clearly strengthen the work.

**If No Observations Pass Filter:**
If all tracked observations fail the quality filter, inform the user that the document is sound and proceed directly to **Offer Final Iteration**.

## Inform the User

After completing document-level and section-level assessment AND applying the **Recommendation Quality Filter** to all tracked observations, inform the user:

"I've identified [X] prioritized observations to address."

Then work through the observations using **Present Recommendations** and **Implement and Loop**. After completing all document-level and section-level observations (all fixes implemented), proceed directly to **Line-Level Assessment**.

## Options Framework (A-E)

**Option A: No Change**
Make no change and move to the next observation.

**Option B: Minimal Change**
The most minimal change possible to address the observation.

**Option C: Moderate Change**
A moderate approach to address the observation.

**Option D: Significant Change**
A substantial revision to address the observation.

**Option E: Park the Idea**
Skip addressing the observation in the current material and instead add it to a parking lot file for future consideration.

## Present Recommendations

For each observation, present a recommendation that includes:
1. Explain the issue clearly (specify document-level/section-level/line-level)
2. Present options A-E with brief descriptions. Only include option E if the observation represents a novel insight worth preserving for future use; otherwise present only options A-D.
3. Provide your recommended option with brief reasoning
4. Ask: "Which option would you prefer?"

**Important**: Each recommendation must have passed through the **Recommendation Quality Filter** before presentation.

## Implement and Loop

1. Implement the user's chosen option
2. Move to the next observation
3. Return to **Present Recommendations**
4. Continue until reaching a re-assessment gate or completing all observations

**Re-Assessment Gates:**

After every 5 implemented changes, pause and ask: "I've made [X] changes. Would you like to continue with remaining observations, re-assess the document with fresh eyes, or stop here?"

Then route based on their response:
- If continue: Proceed to next observation in the queue (return to **Present Recommendations**)
- If re-assess: Return to **Document-Level Content Assessment** to evaluate the revised document
- If stop: Proceed to **Offer Final Iteration**

## Line-Level Assessment

After completing all document-level and section-level observations, begin line-by-line review for:

**AGENTS.md Standards (if in scope):**
- Apply all Enforcement Mechanisms (Bridge Check, Deletion Check)
- Check for Prohibited Elements (style and voice violations)
- Check for AI-signature vocabulary and phrases (see AGENTS.md expanded list)
- Verify Required Structure (openings, body, closings, formatting)
- Reference AGENTS.md for complete criteria

**voice.md Standards (if in scope):**
- Verify Core Principles alignment
- Check Signature Moves application
- Verify Person Switching consistency
- Ensure Authenticity Constraints compliance
- Apply Flow Checks
- Reference voice.md for complete criteria

**Voice Pattern Checks (testable criteria from voice.md):**
- [ ] Opening contains action, tension, or claim (no context-setting, no throat-clearing)
- [ ] No three consecutive sentences within 5 words of each other in length
- [ ] "Because," "therefore," "which means," or "Why?" appears at least once per major point
- [ ] Abstract claims grounded with concrete image within 2 sentences
- [ ] Pattern names (if used) include mechanism explanations within 2 sentences
- [ ] Person switching consistent (first person for experience, third for methods)
- [ ] Context register appropriate (narrative/analytical/declarative/instructional)
- [ ] Compare against voice.md: Does this match the demonstrated patterns?

**Flow and Clarity:**
- Adjacent paragraph connections
- Sentence-level flow and clarity
- Over-repetition of ideas

**Consistency:**
- Terminology consistency
- Local contradictions or confusion

**Assessment Process:** Review the entire document line-by-line against all relevant criteria above. As you identify observations, track them in a working list (mental notes, scratch pad, or internal tracking). Once the full line-level assessment is complete, compile and prioritize all tracked observations, then present them one at a time.

**Automatic Fixes:** Some observations require no subjective judgment and should be fixed automatically without presenting to the user:
- Objective AGENTS.md/voice.md violations with no subjectivity (reference AGENTS.md for specific prohibited elements)
- Factual errors (version numbers, broken references)
- Formatting violations (reference AGENTS.md for formatting standards)

Fix these automatically and continue. Do not present them to the user.

Complete the full line-level assessment, identify all observations, prioritize by urgency and impact, then proceed to **Present Recommendations** to present and resolve them sequentially using the same observation-presentation mechanism as document-level/section-level observations.

**Prioritization criteria:**
1. Issues that create confusion or misinterpretation
2. AGENTS.md/voice.md violations (if in scope)
3. Flow and transition problems
4. Terminology inconsistencies
5. Minor refinements

# PART IV: FINAL QUALITY CHECKS

## Over-Engineering Review

**Purpose**: After implementing all document-level, section-level, and line-level changes, systematically evaluate the revised document to identify where improvements may have over-complicated, over-explained, or added unnecessary complexity.

**Critical Context**: You are reviewing the POST-CHANGE document, not the original. Look specifically for complexity introduced by the review process itself.

**Over-Complication Assessment:**
- Where did adding precision make things harder to follow?
- Did we add unnecessary complexity relative to value?
- Where did format specifications become excessive?

**Over-Explanation Assessment:**
- Where did we add explanatory scaffolding that should be cut?
- Did we make explicit something better left implicit?
- Where does redundancy dilute rather than reinforce?
- Did additions create throat-clearing or unnecessary setup?
- Where are we explaining the explanation rather than delivering value?

**Added Complexity vs. Value:**
- Which additions strengthen the work vs. just make it more thorough?
- Where did completeness sacrifice usability?
- Would a simpler version be more effective even if less comprehensive?
- What would we cut if forced to reduce length by 20%?

**Synthesis Opportunities Missed:**
- Did our additions create new opportunities for synthesis?
- Can newly added content consolidate with existing content?
- Where could 3 added sentences become 1 stronger sentence?

**Assessment Process:** Review the entire revised document against all criteria above. As you identify simplification opportunities, track them in a working list (mental notes, scratch pad, or internal tracking). Once the full over-engineering assessment is complete, compile and prioritize all tracked observations by potential impact on clarity and usability.

**Prioritization criteria:**
1. Complexity that blocks understanding
2. Over-explanation that dilutes core message
3. Redundancy introduced by our changes
4. Synthesis opportunities to consolidate additions

**If observations are identified:** Present them one at a time using **Present Recommendations** and **Implement and Loop**. After implementing simplifications, proceed to **Quality Verification Check**.

**If no observations are identified:** The document does not suffer from over-engineering. Proceed directly to **Quality Verification Check**.

## Quality Verification Check

Before offering final iteration, perform a critical self-assessment:

**"Did our changes actually improve the content?"**

Compare the revised material against the original, evaluating:

**Overall Coherence:**
- Does the revised version flow better or did we break working transitions?
- Is the narrative arc stronger or fragmented?
- Did synthesis improve clarity or create new confusion?

**Voice Consistency:**
- Did changes maintain consistent voice throughout or create tonal shifts?
- Are new additions stylistically aligned with preserved sections?

**Introduced Problems:**
- Did fixes in one area create new issues elsewhere?
- Are there new contradictions or inconsistencies between changed sections?
- Did we over-explain something that was clearer before?
- Did restructuring move technical terms before their grounding/definition?
- Are previously-grounded terms now appearing ungrounded on first use?
- Did section reordering break the sequential logic of term introduction?

**Value Trade-offs:**
- Did cuts remove scaffolding or remove substance?
- Did additions strengthen arguments or dilute impact?
- Is the revised version genuinely stronger or just different?

**Cumulative Effect:**
- Do all changes work together coherently?
- Did multiple small changes create an unintended cumulative effect?

**If quality issues are identified:**
1. Present them to the user as new observations using **Present Recommendations** and **Implement and Loop** (these are unintended issues introduced by earlier changes)
2. After implementing fixes, re-run this quality verification check
3. Only proceed to **Offer Final Iteration** when verification confirms changes improved the material

**If verification confirms improvement:**
Proceed to **Offer Final Iteration**.

## Offer Final Iteration

After completing all observations or stopping at user request:

"Would you like to run the content review again on the newly revised material with fresh eyes?"

Continue iterating until either:
- No more observations are identified, OR
- User indicates the material is ready